# ğŸ¤– Codebase RAG Assistant

Ask questions about any codebase using AI-powered semantic search. This tool uses **Retrieval-Augmented Generation (RAG)** to provide accurate, context-aware answers about your code.

## âœ¨ Features

- ğŸ” **Semantic Code Search** - Find relevant code using natural language
- ğŸ¤– **AI-Powered Answers** - Get explanations using CodeLlama
- ğŸ“¦ **100% Local** - No cloud calls, your code stays private
- ğŸš€ **Fast Indexing** - FAISS vector search for instant retrieval
- ğŸ’» **Clean UI** - Beautiful Streamlit interface
- ğŸ”Œ **GitHub Integration** - Index any public repository

## ğŸ¯ Use Cases

- "How does authentication work in this codebase?"
- "Explain the database connection logic"
- "What does the UserService class do?"
- "Show me all API endpoints"
- "Where is error handling implemented?"

## ğŸ› ï¸ Installation

### Prerequisites

1. **Python 3.8+**
2. **Ollama** - [Download here](https://ollama.com/download)

### Setup

1. **Clone the repository:**
```bash
git clone https://github.com/harshitak4/codebase-rag.git
cd codebase-rag
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Install and start Ollama:**
```bash
# Download Ollama from https://ollama.com/download
# Then pull the CodeLlama model:
ollama pull codellama:7b
```

4. **Start Ollama server:**
```bash
ollama serve
```

## ğŸš€ Quick Start

### Option 1: Index a GitHub Repository

```bash
python -m app.build_index --github https://github.com/pallets/flask
```

### Option 2: Index a Local Repository

```bash
python -m app.build_index --local /path/to/your/repo
```

### Launch the UI

```bash
streamlit run ui/streamlit_app.py
```

Open http://localhost:8501 in your browser.

## ğŸ“– Usage Examples

### Building an Index

**From GitHub:**
```bash
python -m app.build_index --github https://github.com/tiangolo/fastapi
```

**From Local Path:**
```bash
python -m app.build_index --local ./my-project
```

### Using the CLI

```bash
python test_rag.py
```

### Using the Web UI

1. Start Streamlit: `streamlit run ui/streamlit_app.py`
2. Enter your question
3. Click "Ask"
4. View AI-generated answer and source code

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GitHub    â”‚
â”‚  Repository â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Code Ingestion  â”‚
â”‚  (AST Parser)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Code Chunks      â”‚
â”‚ (Functions/      â”‚
â”‚  Classes)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embeddings       â”‚
â”‚ (SentenceTrans-  â”‚
â”‚  former)         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FAISS Index      â”‚
â”‚ (Vector Store)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Question    â”‚â”€â”€â”€â”€â”€â–¶â”‚   Search    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  Retrieved   â”‚
                          â”‚    Code      â”‚
                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Ollama     â”‚
                          â”‚ (CodeLlama)  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚    Answer    â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
codebase-rag/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ vector_store.py       # FAISS vector store
â”‚   â”œâ”€â”€ ingest_code.py         # Code extraction (AST)
â”‚   â”œâ”€â”€ ingest_github_repo.py  # GitHub cloning
â”‚   â”œâ”€â”€ build_index.py         # Index building pipeline
â”‚   â””â”€â”€ rag_answer.py          # RAG system with Ollama
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py       # Web interface
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ repos/                 # Cloned repositories
â”‚   â””â”€â”€ code_index/            # FAISS index + metadata
â”œâ”€â”€ test_rag.py                # CLI test script
â”œâ”€â”€ requirements.txt           # Dependencies
â””â”€â”€ README.md                  # This file
```

## ğŸ”§ Configuration

### Change the LLM Model

Edit `app/rag_answer.py`:
```python
# Default: codellama:7b
# Other options: codellama:13b, deepseek-coder, starcoder
rag = RAGAnswerer(model="deepseek-coder")
```

### Adjust Retrieval Count

In the Streamlit UI sidebar, use the slider to change the number of code chunks retrieved (default: 5).

### File Size Limits

Edit `app/ingest_code.py`:
```python
MAX_FILE_SIZE_KB = 500        # Skip files larger than this
MAX_CHARS_PER_FILE = 100000   # Character limit per file
```

## ğŸ§ª Testing

Run the test script:
```bash
python test_rag.py
```

This will:
1. Load the index
2. Ask 3 test questions
3. Show answers and retrieved code

## ğŸ› Troubleshooting

### "Index not found"
**Solution:** Build an index first using `python -m app.build_index`

### "Ollama not running"
**Solution:** Start Ollama with `ollama serve`, then pull the model with `ollama pull codellama:7b`

### "No code chunks found"
**Solution:** The repository might not have any Python files, or they're all being filtered out. Check the ignore lists in `app/ingest_code.py`

### Slow performance
**Solution:** 
- Use a smaller model: `codellama:7b` instead of `codellama:13b`
- Reduce retrieval count (k parameter)
- Index fewer files

## ğŸ“ How It Works

1. **Code Ingestion**: Python files are parsed using AST to extract functions and classes
2. **Embedding Generation**: Each code chunk is converted to a 384-dim vector using sentence-transformers
3. **Vector Indexing**: Vectors are stored in a FAISS index for fast similarity search
4. **Query Processing**: User questions are embedded and searched against the index
5. **Context Retrieval**: Top-k most similar code chunks are retrieved
6. **Answer Generation**: Retrieved code + question are sent to Ollama (CodeLlama)
7. **Response**: AI generates a contextual answer based on actual code

## ğŸ“Š Performance

- **Indexing Speed**: ~100 files/minute (depends on file size)
- **Search Latency**: <100ms for retrieval
- **Answer Generation**: 5-15 seconds (depends on model and hardware)
- **Memory Usage**: ~2GB RAM for small repos, ~5GB for large repos

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- Support for more languages (JavaScript, Java, etc.)
- Better code chunking strategies
- Web-based index building UI
- Multi-repo indexing
- Code similarity visualization

## ğŸ“ License

MIT License - feel free to use this for any purpose.

##  Acknowledgments

- **FAISS** by Meta AI
- **sentence-transformers** by UKPLab
- **Ollama** for easy local LLM deployment
- **Streamlit** for the awesome UI framework

## ğŸ“§ Contact

Questions? Open an issue on GitHub!

---

**Built with â¤ï¸ for developers who want to understand codebases faster**
